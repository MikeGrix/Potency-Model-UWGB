---
title: "Data Modeling"
author: "Michael Grix"
date: "2024-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r includes, message=FALSE}
library(ISLR)
library(ggformula)

library(caret)  # for data splitting and modeling
library(e1071)  # for SVR
library(randomForest)  # for Random Forest Regression
library(xgboost)

library(nnet)
library(ggplot2)

```

```{r}
# Load the dataset
data <- read.csv("./Clean Data/model_data.csv", header = TRUE, sep = ",")

data$log_corrected_potency <- log(data$corrected_potency)


columns_to_use <- c("Production.Set.Point.A","Production.Set.Point.B","Production.Set.Point.C","Production.Set.Point.D","Growth.Time.A","Growth.Slope.A","Growth.Slope.B","Growth.Max","Growth.Slope.C","Growth.Time.B",
"Protein.Conc.A","Protein.C","Protein.Conc.D","Adjuvent.Conc","Protein.Conc.B_KNNimputed","Protein.Conc.C_KNNimputed", "Vacc_age")


# Split data into training and testing sets (80% training, 20% testing)
set.seed(123)  # for reproducibility
train_index <- createDataPartition(data$log_corrected_potency, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]


# Subset the data for training and testing
train_data <- train_data[, c(columns_to_use, "log_corrected_potency")]
test_data <- test_data[, c(columns_to_use, "log_corrected_potency")]


```

```{r}
# Train SVR model using selected columns
svr_model <- svm(log_corrected_potency ~ ., data = train_data, type = "eps-regression")

# Predict on the test set
svr_predictions <- predict(svr_model, test_data)

# Calculate RMSE for SVR
svr_rmse <- sqrt(mean((test_data$log_corrected_potency - svr_predictions)^2))
print(paste("SVR RMSE:", svr_rmse))

```


```{r}
# Define the tuning grid
tune_grid <- expand.grid(
  mtry = c(2, 3, 4)  # Try different numbers of predictors
)

# Define cross-validation settings (5-fold in this example)
train_control <- trainControl(
  method = "cv",         # Cross-validation
  number = 5,            # 5-fold cross-validation
  verboseIter = TRUE     # To see progress
)

set.seed(123)  # For reproducibility

# Train the Random Forest model with cross-validation
tuned_rfr <- train(
  log_corrected_potency ~ .,      # Formula for target and predictors
  data = train_data,          # Training dataset
  method = "rf",              # Random Forest method
  trControl = train_control,  # Cross-validation control
  tuneGrid = tune_grid,       # Hyperparameter grid
  ntree = 1000                 # Set the number of trees
)

# View the results of tuning
print(tuned_rfr)


# Get the best tuned model's hyperparameters
best_mtry <- tuned_rfr$bestTune$mtry
print(paste("Best mtry:", best_mtry))

# Best RMSE from cross-validation
best_rmse <- min(tuned_rfr$results$RMSE)
print(paste("Best RMSE from cross-validation:", best_rmse))


# Make predictions on the test set using the best model
rfr_predictions <- predict(tuned_rfr, test_data)

# Calculate RMSE on the test set
test_rmse <- sqrt(mean((test_data$log_corrected_potency - rfr_predictions)^2))
print(paste("Test RMSE after tuning:", test_rmse))

# Plot the tuning results
plot(tuned_rfr)

# Plot feature importance
varImpPlot(tuned_rfr$finalModel)



# Make predictions on the test set using the tuned model
rfr_predictions <- predict(tuned_rfr, test_data)

# Combine observed and predicted values into a data frame for plotting
results_df <- data.frame(
  Observed = test_data$log_corrected_potency,  # Replace with your actual target variable name
  Predicted = rfr_predictions
)
```
```{r}
ggplot(results_df, aes(x = Observed, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +   # Scatter plot
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Reference line
  labs(title = "Observed vs. Predicted Potency",
       x = "Observed Potency",
       y = "Predicted Potency") +
  theme_minimal()  # Use a minimal theme for a cleaner look
```
```{r}
# Prepare the training data
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, -which(names(train_data) == "log_corrected_potency")]),  # All columns except the target
                      label = train_data$log_corrected_potency)  # Target variable (log-transformed potency)

# Prepare the test data
dtest <- xgb.DMatrix(data = as.matrix(test_data[, -which(names(test_data) %in% c("corrected_potency", "log_corrected_potency"))]),  # All columns except the targets
                     label = test_data$log_corrected_potency)  # Use log_corrected_potency for evaluation


# Define hyperparameter grid
tune_grid <- expand.grid(
  nrounds = c(100, 200, 300),             # Number of boosting iterations
  max_depth = c(3, 5, 7),                 # Maximum depth of trees
  eta = c(0.01, 0.1, 0.2),                # Learning rate
  gamma = c(0, 1, 5),                     # Minimum loss reduction required to make a further partition
  colsample_bytree = c(0.6, 0.8, 1.0),   # Subsample ratio of columns when constructing each tree
  min_child_weight = c(1, 3, 5),          # Minimum sum of instance weight needed in a child
  subsample = c(0.6, 0.8, 1.0)            # Subsample ratio of the training instances
)

# Define control for training
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

set.seed(123)  # For reproducibility

# Suppress warnings temporarily during model training
suppressWarnings({
  xgb_model <- train(
    log_corrected_potency ~ .,              # Use the log-transformed potency variable
    data = train_data,                      # Training dataset
    method = "xgbTree",                     # XGBoost method
    trControl = train_control,              # Cross-validation control
    tuneGrid = tune_grid                    # Hyperparameter grid
  )
})


xgb_predictions <- predict(xgb_model, newdata = test_data)

# Calculate RMSE
test_rmse <- sqrt(mean((test_data$log_corrected_potency - xgb_predictions)^2))
print(paste("Test RMSE:", test_rmse))


# Combine observed and predicted values into a data frame for plotting
results_df <- data.frame(
  Observed = test_data$log_corrected_potency,  # Original observed potency
  Predicted = xgb_predictions                 # Inverse-transformed predictions
)

# Create the plot
ggplot(results_df, aes(x = Observed, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +   # Scatter plot
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +  # Reference line
  labs(title = "Observed vs. Predicted Potency (XGBoost)",
       x = "Observed Potency",
       y = "Predicted Potency") +
  theme_minimal()


```
```{r}
# Normalize or scale numeric columns in training and test sets
preprocess_params <- preProcess(train_data, method = c("center", "scale"))
train_data <- predict(preprocess_params, train_data)
test_data <- predict(preprocess_params, test_data)

# Define hyperparameter grid for tuning
tune_grid <- expand.grid(
  size = c(3, 5, 7),        # Number of units in the hidden layer
  decay = c(0.1, 0.01, 0.001)  # Regularization parameter to avoid overfitting
)

# Set up cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

# Train the ANN model
set.seed(123)  # For reproducibility
ann_model <- train(
  log_corrected_potency ~ .,      # Target and predictors
  data = train_data,              # Training dataset
  method = "nnet",                # Neural network method in caret
  trControl = train_control,      # Cross-validation control
  tuneGrid = tune_grid,           # Hyperparameter grid
  linout = TRUE                   # For regression output (continuous)
)


# Predict on the test set
ann_predictions <- predict(ann_model, newdata = test_data)

# Inverse the log transformation to compare with original potency values


# Calculate RMSE on the test set
test_rmse <- sqrt(mean((test_data$log_corrected_potency - ann_predictions)^2))
print(paste("Test RMSE:", test_rmse))

# Prepare data for plotting
results_df <- data.frame(
  Observed = test_data$log_corrected_potency,
  Predicted = ann_predictions
)

# Plot observed vs. predicted potency
library(ggplot2)
ggplot(results_df, aes(x = Observed, y = Predicted)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'red', linetype = 'dashed') +
  labs(title = "Observed vs. Predicted Potency (ANN)",
       x = "Observed Potency",
       y = "Predicted Potency") +
  theme_minimal()

```


```{r}
# Predict the mean of Potency for all test samples
mean_potency <- mean(train_data$log_corrected_potency, na.rm = TRUE)

# Create predictions using the mean
mean_predictions <- rep(mean_potency, nrow(test_data))

# Calculate RMSE for the mean baseline
mean_rmse <- sqrt(mean((test_data$log_corrected_potency - mean_predictions)^2))
print(paste("Mean Baseline RMSE:", mean_rmse))

# Predict the median of Potency for all test samples
median_potency <- median(train_data$log_corrected_potency, na.rm = TRUE)

# Create predictions using the median
median_predictions <- rep(median_potency, nrow(test_data))

# Calculate RMSE for the median baseline
median_rmse <- sqrt(mean((test_data$log_corrected_potency - median_predictions)^2))
print(paste("Median Baseline RMSE:", median_rmse))

```

