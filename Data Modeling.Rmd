---
title: "Data Modeling"
author: "Michael Grix"
date: "2024-10-24"
output: html_document
---

```{r includes, message=FALSE}
library(ISLR)
library(ggformula)
library(corrplot)
library(dplyr)
library(reshape2)

library(caret)  # for data splitting and modeling
library(e1071)  # for SVR
library(randomForest)  # for Random Forest Regression
library(xgboost)

library(nnet)
library(ggplot2)

library(DiagrammeR)

```



```{r}

data <- read.csv("./Clean Data/model_data.csv", header = TRUE, sep = ",")


columns_to_use <- c("Family","Production.Set.Point.B","Production.Set.Point.C","Production.Set.Point.D","Growth.Time.A","Growth.Slope.A","Growth.Slope.B","Growth.Max","Growth.Slope.C","Growth.Time.B","Protein.Conc.A","Protein.C","Protein.Conc.D","Adjuvent.Conc","Protein.Conc.B_KNNimputed","Protein.Conc.C_KNNimputed", "Vacc_age")


# Split data into training and testing sets (80% training, 20% testing)
set.seed(645)  # for reproducibility
train_index <- createDataPartition(data$log_corrected_potency, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]


# Subset the data for training and testing
train_data <- train_data[, c(columns_to_use, "log_corrected_potency")]
test_data <- test_data[, c(columns_to_use,"log_corrected_potency", "corrected_potency")]

```

```{r}
train_control <- trainControl(method = "cv", number = 5)  # cross-validation

svr_model <- train(
  log_corrected_potency ~ .,
  data = train_data,
  method = "svmRadial",
  trControl = train_control
)

# Predict on the test set
svr_predictions <- predict(svr_model, test_data)

# Un-log the predicted results
svr_predictions_original_scale <- exp(svr_predictions)


# Calculate RMSE for SVR
svr_rmse <- sqrt(mean((test_data$corrected_potency - svr_predictions_original_scale)^2))
svr_mae <- mean(abs(test_data$corrected_potency - svr_predictions_original_scale))

```


```{r}
# Define the tuning grid
tune_grid <- expand.grid(
  mtry = c(5,6,7)  # Try different numbers of predictors
)

# Define cross-validation settings (5-fold in this example)
train_control <- trainControl(
  method = "cv",         # Cross-validation
  number = 5,            # 5-fold cross-validation
  verboseIter = TRUE     # To see progress
)

#set.seed(123)  # For reproducibility

# Train the Random Forest model with cross-validation
tuned_rfr <- train(
  log_corrected_potency ~ .,      # Formula for target and predictors
  data = train_data,          # Training dataset
  method = "rf",              # Random Forest method
  trControl = train_control,  # Cross-validation control
  tuneGrid = tune_grid,       # Hyperparameter grid
  ntree = 1000                 # Set the number of trees
)

# View the results of tuning
print(tuned_rfr)


# Get the best tuned model's hyperparameters
best_mtry <- tuned_rfr$bestTune$mtry
print(paste("Best mtry:", best_mtry))

# Best RMSE from cross-validation
best_rmse <- min(tuned_rfr$results$RMSE)
print(paste("Best RMSE from cross-validation:", best_rmse))


# Make predictions on the test set using the best model
rfr_predictions <- predict(tuned_rfr, test_data)

rfr_predictions_original_scale = exp(rfr_predictions)

# Calculate RMSE on the test set
rfr_rmse <- sqrt(mean((test_data$corrected_potency - rfr_predictions_original_scale)^2))
rfr_mae <- mean(abs(test_data$corrected_potency - rfr_predictions_original_scale))

print(paste("Test RMSE after tuning:", rfr_rmse))

```



```{r}
# Prepare the training data
# Convert Family column from A/B to 1/2
xgb_train_data <- train_data %>%
  mutate(Family = ifelse(Family == "A", 1, 2))


dtrain <- xgb.DMatrix(data = as.matrix(xgb_train_data[, -which(names(xgb_train_data) == "log_corrected_potency")]),  # All columns except the target
                      label = xgb_train_data$log_corrected_potency)  # Target variable (log-transformed potency)

# Prepare the test data
xgb_test_data <- test_data %>%
  mutate(Family = ifelse(Family == "A", 1, 2))

dtest <- xgb.DMatrix(data = as.matrix(xgb_test_data[, -which(names(xgb_test_data) %in% c("corrected_potency", "log_corrected_potency"))]),  # All columns except the targets
                     label = xgb_test_data$log_corrected_potency)  # Use log_corrected_potency for evaluation


# Define hyperparameter grid
tune_grid <- expand.grid(
  nrounds = c(50, 200),             # Number of boosting iterations
  max_depth = c(3, 5),                 # Maximum depth of trees
  eta = c(0.05, 0.3),                # Learning rate
  gamma = c(0, 1),                     # Minimum loss reduction required to make a further partition
  colsample_bytree = c(0.6, 0.8),   # Subsample ratio of columns when constructing each tree
  min_child_weight = c(1, 3, 5),          # Minimum sum of instance weight needed in a child
  subsample = c(0.6, 0.8, 1.0)            # Subsample ratio of the training instances
)


# Define control for training
train_control <- trainControl(method = "cv", number = 3)  # 5-fold cross-validation

#set.seed(123)  # For reproducibility

xgb_model <- train(
    log_corrected_potency ~ .,              # Use the log-transformed potency variable
    data = train_data,                      # Training dataset
    method = "xgbTree",                     # XGBoost method
    trControl = train_control,              # Cross-validation control
    tuneGrid = tune_grid,                    # Hyperparameter grid
    verbosity = 0
)


xgb_predictions <- predict(xgb_model, newdata = test_data)
xgb_predictions_original_scale = exp(xgb_predictions)

# Calculate RMSE
xgb_rmse <- sqrt(mean((test_data$corrected_potency - xgb_predictions_original_scale)^2))
xgb_mae <- mean(abs(test_data$corrected_potency - xgb_predictions_original_scale))


xgb_feats = xgb.importance(model = xgb_model$finalModel)

```


```{r}
# Normalize or scale numeric columns in training and test sets
preprocess_params <- preProcess(train_data, method = c("center", "scale"))
ann_train_data <- predict(preprocess_params, train_data)

test_data$log_corrected_potency <- log(test_data$corrected_potency)

data_temp <- test_data[, !names(test_data) %in% "corrected_potency"]

ann_test_data <- predict(preprocess_params, data_temp)


# Define hyperparameter grid for tuning
tune_grid <- expand.grid(
  size = c(3, 5, 7),        # Number of units in the hidden layer
  decay = c(0.1, 0.01, 0.001)  # Regularization parameter to avoid overfitting
)

# Set up cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE
)

# Train the ANN model
set.seed(123)  # For reproducibility
ann_model <- train(
  log_corrected_potency ~ .,      # Target and predictors
  data = train_data,              # Training dataset
  method = "nnet",                # Neural network method in caret
  trControl = train_control,      # Cross-validation control
  tuneGrid = tune_grid,           # Hyperparameter grid
  maxit = 5000,
  linout = TRUE                   # For regression output (continuous)
)


# Predict on the test set
ann_predictions <- predict(ann_model, newdata = test_data)

# Inverse the log transformation to compare with original potency values
ann_predictions_original_scale = exp(ann_predictions)

# Calculate RMSE on the test set
ann_rmse <- sqrt(mean((test_data$corrected_potency - ann_predictions_original_scale)^2))
ann_mae <- mean(abs(test_data$corrected_potency - ann_predictions_original_scale))

```


```{r baseline}
# Predict the mean of Potency for all test samples
mean_potency <- mean(train_data$log_corrected_potency, na.rm = TRUE)

# Create predictions using the mean
mean_predictions <- rep(mean_potency, nrow(test_data))

# Inverse the log transformation to compare with original potency values
mean_predictions_original_scale = exp(mean_predictions)


# Calculate RMSE for the mean baseline
mean_rmse <- sqrt(mean((test_data$corrected_potency - mean_predictions_original_scale)^2))
mean_mae <- mean(abs(test_data$corrected_potency - mean_predictions_original_scale))

print(paste("Mean Baseline RMSE:", mean_rmse))

# Predict the median of Potency for all test samples
median_potency <- median(train_data$log_corrected_potency, na.rm = TRUE)

# Create predictions using the median
median_predictions <- rep(median_potency, nrow(test_data))

# Inverse the log transformation to compare with original potency values
median_predictions_original_scale = exp(median_predictions)

# Calculate RMSE for the median baseline
median_rmse <- sqrt(mean((test_data$corrected_potency - median_predictions_original_scale)^2))
median_mae <- mean(abs(test_data$corrected_potency - median_predictions_original_scale))

print(paste("Median Baseline RMSE:", median_rmse))

```

```{r graphs}

# Define RMSE values for each model
rmse_values <- data.frame(
  Model = c("Mean Baseline", "Median Baseline", "SVR", "RFR", "XGBoost", "ANN"),
  RMSE = c(mean_rmse, median_rmse, svr_rmse, rfr_rmse, xgb_rmse, ann_rmse)
)

# Convert 'Model' to a factor and set levels to control the display order
rmse_values$Model <- factor(rmse_values$Model, levels = c("Mean Baseline", "Median Baseline", "SVR", "RFR", "XGBoost", "ANN"))

# Create bar plot
ggplot(rmse_values, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.6) +
  labs(title = "RMSE Comparison of Models",
       x = "Model",
       y = "RMSE") +
  theme_minimal() +
  theme(legend.position = "none") +
  geom_text(aes(label = round(RMSE, 2)), vjust = -0.5, size = 3.5)


# Define RMSE values for each model
mae_values <- data.frame(
  Model = c("Mean Baseline", "Median Baseline", "SVR", "RFR", "XGBoost", "ANN"),
  MAE = c(mean_mae, median_mae, svr_mae, rfr_mae, xgb_mae, ann_mae)
)

mae_values$Model <- factor(mae_values$Model, levels = c("Mean Baseline", "Median Baseline", "SVR", "RFR", "XGBoost", "ANN"))

# Create bar plot
ggplot(mae_values, aes(x = Model, y = MAE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.6) +
  labs(title = "MAE Comparison of Models",
       x = "Model",
       y = "MAE") +
  theme_minimal() +
  theme(legend.position = "none") +
  geom_text(aes(label = round(MAE, 2)), vjust = -0.5, size = 3.5)
```



```{r}


test_data <- test_data %>% 
  mutate(SVR = svr_predictions_original_scale, RFR = rfr_predictions_original_scale, XGB = xgb_predictions_original_scale, ANN = ann_predictions_original_scale, mean = mean_predictions_original_scale)

# Plot data
ggplot(test_data, aes(x = corrected_potency)) +
  # Points and fit lines for each model
  geom_point(aes(y = SVR, color = "SVR")) +
  geom_smooth(aes(y = SVR, color = "SVR"), method = "lm", se = FALSE) +
  
  geom_point(aes(y = RFR, color = "RFR")) +
  geom_smooth(aes(y = RFR, color = "RFR"), method = "lm", se = FALSE) +
  
  geom_point(aes(y = XGB, color = "XGB")) +
  geom_smooth(aes(y = XGB, color = "XGB"), method = "lm", se = FALSE) +
  
  geom_point(aes(y = ANN, color = "ANN")) +
  geom_smooth(aes(y = ANN, color = "ANN"), method = "lm", se = FALSE) +
  
  # Identity line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  
  # Color and labeling adjustments
  scale_color_manual(
    values = c("SVR" = "#00AFBB", "RFR" = "#E7B80D", "XGB" = "#FC4E07", "ANN" = "#663399"),
    labels = c("SVR", "RFR", "XGB", "ANN")
  ) +
  labs(x = "Measured Relative Potency", y = "Predicted Realtive Potency") +
  
  # Faceting with free scales for each Family
  facet_grid(~Family, scales = "free") +
  theme_minimal()+
  theme(panel.border = element_rect(color = "black", fill = NA, size = 0.5))


```